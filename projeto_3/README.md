# C√°lculo de m√©tricas de avalia√ß√£o de aprendizado

üìå O resultado das predi√ß√µes do modelo ResNet50 com transfer learning, treinado no conjunto CelebA no projeto_1, foi avaliado a partir das principais m√©tricas de classifica√ß√£o. 

A solu√ß√£o est√° neste diret√≥rio, no notebook do Google Colab [`M√©tricas de valida√ß√£o para modelos de ML.ipynb`](https://github.com/FlaviaLopes/dio-challenges-coding-the-future-with-baires-dev/blob/main/projeto_3/M√©tricas de valida√ß√£o para modelos de ML.ipynb).

$Mas$ $antes$ $disso$... Entenda um pouco mais sobre o desafio.

> **Desafio:** 
"Neste projeto, vamos calcular as principais m√©tricas para avalia√ß√£o de modelos de classifica√ß√£o de dados, como acur√°cia, sensibilidade (recall), especificidade, precis√£o e‚ÄØF1-score. Para que seja poss√≠vel implementar estas fun√ß√µes, voc√™ deve utilizar os m√©todos e suas f√≥rmulas correspondentes (Tabela 1). 
>Para a leitura dos valores de VP, VN, FP e FN, ser√° necess√°rio escolher uma matriz de confus√£o para a base dos c√°lculos. Essa matriz voc√™ pode escolher de forma arbitraria, pois nosso objetivo √© entender como funciona cada m√©trica."

| M√©todo           | F√≥rmula                  |                 										                        
|--------------------|---------------------|
| Sensibilidade     | TP / (TP + FN)     |
| Especificidade   | TN / (FP + TN)    | 
| Acur√°cia	          | (TP + TN) / N      |
| Precis√£o	          | TP / (TP + FP)     |
| F-Score	              | 2 x (PxS) / (P+S) |


üéØ **Etapas**
- Para atingir os objetivos propostos, as seguintes etapas foram realizadas:  
  1. **Escolha da matriz confus√£o:** a matriz √© produto do projeto 1, onde uma rede de classifica√ß√£o multilabel foi treinada com transfer learning usando o dataset CelebA (ver Projeto 1).  
  2. **Implementa√ß√£o das fun√ß√µes para c√°lculo das m√©tricas:** o c√°lculo das m√©tricas foi demonstrado a partir de fun√ß√µes que le√™m a matriz confus√£o multilabel.  
  3. **Uso de fun√ß√µes prontas de bibliotecas para demonstra√ß√£o:** utilizei algumas das fun√ß√µes de c√°lculo de m√©tricas de bibliotecas como sklearn para compara√ß√£o.
  4. **Interpreta√ß√£o das m√©tricas**: a partir das m√©tricas o modelo √© interpretado e interven√ß√µes s√£o sugeridas.

üìù **Contexto**: A se√ß√£o 1, "Entendendo o c√°lculo de m√©tricas de modelos de classifica√ß√£o", d√° mais detalhes sobre as m√©tricas, como s√£o calculadas e relev√¢ncia para avalia√ß√£o dos modelos.

---
üîç **Interpreta√ß√£o do modelo**

__Qual o foco: diminuir FP ou FN?__ 
Neste cen√°rio, o falso positivo √© pior que o falso negativo. Melhorias em todas as m√©tricas importam, mas principalmente em precis√£o e especificidade, pois quanto menos FP, mais pr√≥ximo de 1 ser√£o essas m√©tricas.

__Por quais atributos come√ßar?__ 
Podemos usar como refer√™ncia a pesquisa que publicou este dataset, que j√° fez uma an√°lise explorat√≥ria e trouxe insights importantes sobre os atributos da base: [Deep Learning Face Attributes in the Wild](https://arxiv.org/pdf/1411.7766).

Os autores clusterizaram os pesos da rede para compreender melhor como os atributos estavam se relacionando. E descobriram grupos com rela√ß√£o de co-ocorrr√™ncia forte, e outros cujos atributos tinham em comum as cores ou texturas.

>"As shown in Fig.11, Group #1, Group #2 and Group #4 demonstrate co-occurrence relationship between attributes, e.g. ‚ÄòAttractive‚Äô and ‚ÄòHeavy Makeup‚Äô have high correlation. Attributes in Group #3 share similar color descriptors, while attributes in Group #6 correspond to certain texture and appearance traits."

De acordo com essas informa√ß√µes e an√°lise explorat√≥ria realizada no projeto 1, parece mais razo√°vel come√ßar otimizando o modelo a partir dos atributos com forte co-ocorr√™ncia. Alguns destes, inclusive, s√£o as classes com maior suporte. Uma melhoria nas m√©tricas dessas classes pode levar o modelo a performar melhor.

Nas itera√ß√µes seguintes, a quest√£o dos atributos com cores e texturas semelhantes, e que n√£o performaram bem ap√≥s a primeira otimiza√ß√£o pode ser reavaliada.

__Plano para a pr√≥xima itera√ß√£o__ 

- Focar nos grupos 1, 2 e 4, com forte co-ocorr√™ncia. Preferencialmente os atributos de maior suporte.
- Para cada atributo, ranqueado pelo suporte:
	- Avaliar a matriz de confus√£o da classe, e se houver FP significante:
		- Inspecionar algumas imagens com FP
		- Para as imagens com FP nesta classe, verificar quais outras classes ocorrem.
		- algumas perguntas: os atributos associados a esta classe diferem dos que ocorrem no grupo de FP? pode trazer uma dire√ß√£o mais assertiva para o data augmentation, focando nos exemplos que a rede teve dificuldade de aprender.
		
__Grupos sem√¢nticos__: atributos dos grupos 1, 2 e 4, ordenados por suporte

- atributos considerados:
	- acima de 50% de suporte, em rela√ß√£o a classe maiorit√°ria
	- atributos dos grupos 1, 2 e 4
- atributos selecionados para primeira otimiza√ß√£o: No_Beard, Young, Smiling, Mouth_Slightly_Open, High_Cheekbones, Wearing_Lipstick
- em seguida os demais dos grupos 1, 2 e 4 podem ser avaliados.

__Conclus√£o__

O pr√≥ximo passo √© compreender os falsos positivos. Por exemplo: 
- quais classes ocorrem simultaneamente com as imagens classificadas erroneamente como No_Beard? 
- alguma caracter√≠stica pode estar confundindo o modelo? Esta caracter√≠stica ruidosa tem baixo suporte?
Estas respostas podem indicar a abordagem: oversampling ou undersampling focando em qual grupo de imagens?

---

## Entendendo o c√°lculo de m√©tricas de modelos de classifica√ß√£o
---
Para um problema com `n` classes, analisamos os arrays `y_true` (r√≥tulos verdadeiros) e `y_pred` (r√≥tulos preditos) para entender o desempenho do modelo em rela√ß√£o √†s classes {C1, C2, ..., Cn}. Calculamos as seguintes ocorr√™ncias:
- Verdadeiros Positivos (TP): Quantas vezes a imagem pertencia √† classe 
Ci e foi corretamente classificada como Ci. 
- Falsos Negativos (FN): Quantas vezes a imagem pertencia √† classe 
Ci, mas foi erroneamente classificada como outra classe.
- Falsos Positivos (FP): Quantas vezes a imagem n√£o pertencia √† classe 
Ci, mas foi incorretamente classificada como Ci.
- Verdadeiros Negativos (TN): Quantas vezes a imagem n√£o pertencia √† classe 
Ci e foi corretamente classificada como pertencendo a outra classe.

Essa an√°lise √© fundamental para avaliar o desempenho do modelo em classificar corretamente as classes.

### C√°lculando indicadores a partir da matriz confus√£o
---
A partir da matriz confus√£o de cada classe √© poss√≠vel calcular m√©tricas que resumem o desempenho do modelo por classe. As principais m√©tricas s√£o: i) precis√£o; ii) recall; iii) especificidade; iv) acur√°cia.

1. __Precision__: A precision mede a propor√ß√£o de verdadeiros positivos (TP) em rela√ß√£o ao total de positivos previstos pelo modelo (TP + FP). Avalia a confiabilidade das previs√µes positivas do modelo, minimizando falsos positivos. _F√≥rmula: `TP / (TP + FP)`_.

2. __Recall__: O recall mede a propor√ß√£o de verdadeiros positivos (TP) corretamente identificados em rela√ß√£o ao total de positivos reais. Indica o qu√£o bem o modelo consegue encontrar os exemplos da classe positiva, evitando falsos negativos. _F√≥rmula: `TP / (TP + FN)`_

3. __Specificity__: A especificidade mede a propor√ß√£o de verdadeiros negativos (TN) corretamente identificados em rela√ß√£o ao total de negativos reais. Quanto maior √© a especificidade, melhor o modelo √© em identificar corretamente as inst√¢ncias da classe negativa (ou seja, evitar falsos positivos).
_F√≥rmula: `TN + (TN / FP)`_

4. __Accuracy__: A acur√°cia mede a propor√ß√£o de predi√ß√µes corretas em rela√ß√£o ao total de amostras, considerando todas as classes. Para problemas multilabel, a acur√°cia da classe pode ser calculada como: `(TP + TN) / (TP + FP + FN + TP)`. E a acur√°cia geral √© o somat√≥rio das acur√°cias de cada classe, dividido por n.


__Recall e Specificity (balanceamento entre classes)__
O recall foca nos positivos reais (sensibilidade), enquanto a specificity foca nos negativos reais. Essas duas m√©tricas frequentemente est√£o em conflito. Por exemplo: se focarmos em aumentar o recall, o modelo pode acabar classificando mais negativos como positivos, reduzindo a specificity. Um modelo balanceado deve maximizar ambos para evitar tanto falsos negativos quanto falsos positivos.

__Precision e Specificity (previs√µes positivas e negativas)__
- A m√©trica precision avalia a qualidade das previs√µes positivas, enquanto a specificity avalia a qualidade das previs√µes negativas. 

- Um modelo com alta precision, mas baixa specificity, pode prever corretamente os positivos, mas cometer muitos erros ao classificar negativos como positivos (FP).

- A m√©trica specificity √© essencial em contextos onde falsos positivos t√™m consequ√™ncias graves:
  - na detec√ß√£o de fraudes, um FP pode bloquear uma transa√ß√£o leg√≠tima.
  - em exames de sa√∫de, um FP pode levar a exames desnecess√°rios

- Por outro lado, em alguns cen√°rios o FN √© muito mais grave:
  - detec√ß√£o de c√¢ncer
  - ataques cibern√©ticos

- Ent√£o, se a classe A possui alta precis√£o, significa que o modelo performa com uma baixa taxa de falsos positivos, e se o recall tamb√©m √© alto, significa que o modelo performa com uma baixa taxa de falsos negativos. Quanto maior a precis√£o e o recall, melhor o modelo √© em reconhecer a presen√ßa ou aus√™ncia da classe. Especificidade e sensibilidade equilibradas, s√£o resultado de um modelo balanceado. 

- _A maximiza√ß√£o de uma m√©trica em detrimento de outra vai depender do cen√°rio de aplica√ß√£o._

---
### Combinando m√©tricas
---
__AUC__: especificidade e recall

A curva AUC mede a capacidade do modelo de distinguir entre classes. Para problemas multilabel, a curva AUC √© calculada como a m√©dia das √°reas sob as curvas ROC de cada classe individual:
- A curva ROC (Receiver Operating Characteristic) para cada classe √© gerada tra√ßando o Recall (ou Taxa de Verdadeiros Positivos) contra o 1 - Specificity (ou Taxa de Falsos Positivos).
- Um valor de AUC pr√≥ximo de 1 indica que o modelo √© muito bom em separar positivos de negativos, enquanto um valor pr√≥ximo de 0,5 indica um desempenho semelhante ao acaso.

_F√≥rmula do AUCi (√Årea Sob a Curva ROC para uma classe individual)_
$$
\text{AUC}_i = \int_{0}^{1} \text{TPR}(\text{FPR}) \, d(\text{FPR})
$$

$$
\text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}, \quad \text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}
$$

_Aproxima√ß√£o do AUCi pela soma dos trap√©zios_
$$
\text{AUC}_i \approx \sum_{j=1}^{n-1} \left( \text{FPR}_{j+1} - \text{FPR}_j \right) \cdot \frac{\text{TPR}_j + \text{TPR}_{j+1}}{2}
$$

_AUC m√©dio para problemas multilabel_
$$
\text{AUC}_{\text{multi}} = \frac{1}{n} \sum_{i=1}^{n} \text{AUC}_i
$$


__F1-Score__: precis√£o e recall

O F1-score combina precision e recall, sendo particularmente √∫til para lidar com desequil√≠brios entre as classes. __F√≥rmula__: `2 x (Precision + Recall) / (Precision x Recall)`.
- Para problemas multilabel, o F1-score pode ser calculado de duas formas:
  - Macro: Calcula o F1-score para cada classe individualmente e depois tira a m√©dia, tratando todas as classes igualmente.
  - Micro: Agrega os valores de TP, FP, e FN de todas as classes antes de calcular o F1-score, dando maior peso √†s classes mais frequentes.


---

